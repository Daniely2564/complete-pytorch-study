{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens to Integers\n",
    "- conceptual pseudocode\n",
    "```python\n",
    "dataset = long sequence of words\n",
    "current_idx = 0\n",
    "word2idx = {}\n",
    "for word in dataset:\n",
    "    if word not in word2idx:\n",
    "        word2idx[word] = current_idx\n",
    "        current_idx += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually do not want to start from index 0. We want to reserve 2 spaces. One for padding (empty spots) and one for unknown words. (words from test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant length sequence (at least batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of padding\n",
    "- Post-padding\n",
    "- Pre-padding\n",
    "    - more preferred because of the vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "1. Your data might arrive in an unexpected form. We need to convert to CSV.\n",
    "2. Tokenization\n",
    "3. Map each token to a unique integer\n",
    "4. Add padding (per batch for improved efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal : Text Classification (many-to-one)\n",
    "- input : sequence of words.\n",
    "- output : a single label\n",
    "- example : spam detection\n",
    "- for __torchtext__, we want datast to be a CSV(1 column for label, 1 column for text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as ttd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = ttd.Field(\n",
    "    sequential=True, # The words are sequential\n",
    "    batch_first=True, # N x T, not T x N\n",
    "    lower=True, # Lowercase\n",
    "    pad_first=True # pre-padding\n",
    ")\n",
    "LABEL = ttd.Field(sequential=False, use_vocab=False, is_target=True)\n",
    "# Not sequential Data\n",
    "# use_vocab => Sigining each word to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchtext only knows how to read a tabular dataset\n",
    "dataset = ttd.TabularDataset(\n",
    "    path='spam.csv',\n",
    "    format='csv', # csv, tsv, json\n",
    "    skip_header=True,\n",
    "    fields=[('data',TEXT), ('label', LABEL)]\n",
    ")\n",
    "\n",
    "train_datset, test_dataset = dataset.split(split_ratio=0.7) # .7 by default\n",
    "\n",
    "TEXT.build_vocab(train_dataset) # Assigns a unique integer to each token\n",
    "vocab = TEXT.vocab\n",
    "# has vocab.stoi => string to index\n",
    "# has vocab.itos => index to string\n",
    "# We need this in order to convert back to words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttd.Iterator.splits(\n",
    "    (train_dataset, test_dataset),\n",
    "    sort_key=lambda x: len(x.data),\n",
    "    batch_size=(32, 256), # same number of elements as we have dataset, (count_train, count_test)\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
