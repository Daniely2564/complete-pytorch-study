{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "<img src=\"../img/lstm_im.png/\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "\n",
    "$fn$ = Number of features\n",
    "\n",
    "$hs$ = Number of output nodes (hidden size)\n",
    "\n",
    "$bs$ = Batch size\n",
    "\n",
    "Then:\n",
    " * Each $W_{something}$ matrix below has the shape $(fn, hs)$;\n",
    " * Each $U_{something}$ matrix below has the shape $(hs, hs)$;\n",
    " * Each $b_{something}$ matrix below has the shape $(1, hs)$;\n",
    " * The $x_t$ matrix below has shape $(bs, fn)$, corresponding to the element of index $t$ of each sequence inf the batch.; and\n",
    " * The $h_t$ matrix below has shape $(bs, hs)$, corresponding to hidden state at time $t$ of each sequence inf the batch.\n",
    "\n",
    "And:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_t = \\sigma(W_f \\ x_t + U_f \\ h_{t-1} + b_f)$\n",
    "\n",
    "$i_t = \\sigma(W_i \\ x_t + U_i \\ h_{t-1} + b_i)$\n",
    "\n",
    "$o_t = \\sigma(W_o \\ x_t + U_o \\ h_{t-1} + b_o)$\n",
    "\n",
    "$g_t = \\tanh \\ (W_g \\ x_t + U_g \\ h_{t-1} + b_g)$ a.k.a. $\\tilde{c}_t$\n",
    "\n",
    "$c_t = f_t \\circ c_{t-1} + i_t \\circ g_t$\n",
    "\n",
    "$h_t = o_t \\circ \\tanh \\ (c_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, device):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.device = device\n",
    "        self.W_xf, self.W_hf, self.b_f = torch.randn(input_size, hidden_size), torch.randn(hidden_size, hidden_size), torch.zeros(hidden_size)\n",
    "        self.W_xi, self.W_hi, self.b_i = torch.randn(input_size, hidden_size), torch.randn(hidden_size, hidden_size), torch.zeros(hidden_size)\n",
    "        self.W_xo, self.W_ho, self.b_o = torch.randn(input_size, hidden_size), torch.randn(hidden_size, hidden_size), torch.zeros(hidden_size)\n",
    "        self.W_xc, self.W_hc, self.b_c = torch.randn(input_size, hidden_size), torch.randn(hidden_size, hidden_size), torch.zeros(hidden_size)\n",
    "        \n",
    "    def forward(self, X, state):\n",
    "        prev_h, prev_c = state\n",
    "        batch_size, timesteps, _  = X.shape\n",
    "        hidden_seq = []\n",
    "        for t in range(timesteps):\n",
    "            x = X[:,t,:]\n",
    "            forget_gate = torch.sigmoid(x @ self.W_xf + prev_h @ self.W_hf + self.b_f)\n",
    "            input_gate = torch.sigmoid(x @ self.W_xi + prev_h @ self.W_hf + self.b_i)\n",
    "            output_gate = torch.sigmoid(x @ self.W_xo + prev_h @ self.W_ho + self.b_o)\n",
    "            C_tilda = torch.tanh(x @ self.W_xc + prev_h @ self.W_hc + self.b_c)\n",
    "            Ct = forget_gate * prev_c + input_gate * C_tilda\n",
    "            Ht = output_gate * torch.tanh(Ct)\n",
    "            hidden_seq.append(Ht.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.permute(1, 0, 2)\n",
    "        \n",
    "        return hidden_seq, hidden_seq_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, device):\n",
    "    super(LSTM, self).__init__()\n",
    "    self.device = device\n",
    "    self.params = self.init_params(input_size, hidden_size)\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      input_size: int, feature dimension of input sequence\n",
    "      hidden_size: int, feature dimension of hidden state\n",
    "      device: torch.device()\n",
    "    \"\"\"\n",
    "  \n",
    "  def init_params(self, input_size, hidden_size):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      input_size: int, feature dimension of input sequence\n",
    "      hidden_size: int, feature dimension of hidden state\n",
    "      \n",
    "    Outputs:\n",
    "      Weights for proposal: W_xc, W_hc, b_c\n",
    "      Weights for input gate: W_xi, W_hi, b_i\n",
    "      Weights for forget gate: W_xf, W_hf, b_f\n",
    "      Weights for output gate: W_xo, W_ho, b_o\n",
    "    \"\"\"\n",
    "    W_xc, W_hc, b_c = None, None, None\n",
    "    W_xi, W_hi, b_i = None, None, None\n",
    "    W_xf, W_hf, b_f = None, None, None\n",
    "    W_xo, W_ho, b_o = None, None, None\n",
    "    ##############################################################################\n",
    "    # TODO: Initialize the weights and biases. The result will be stored in \n",
    "    # `params` below. Weights should be initialized using `torch.randn` multiplied \n",
    "    # with the scale (0.1). Biases should be initialized to 0.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    D, M = input_size, hidden_size\n",
    "    self.hidden_size = hidden_size\n",
    "    W_xc, W_hc, b_c = torch.randn(D,M) * 0.1, torch.randn(M,M) * 0.1, torch.zeros(M,)\n",
    "    W_xi, W_hi, b_i = torch.randn(D,M) * 0.1, torch.randn(M,M) * 0.1, torch.zeros(M,)\n",
    "    W_xf, W_hf, b_f = torch.randn(D,M) * 0.1, torch.randn(M,M) * 0.1, torch.zeros(M,)\n",
    "    W_xo, W_ho, b_o = torch.randn(D,M) * 0.1, torch.randn(M,M) * 0.1, torch.zeros(M,) \n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    params = [W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o]\n",
    "    return params\n",
    "\n",
    "  \n",
    "  def lstm(self, X, state):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      X: tuple of tensors (src, src_len). src, size (N, D_in) or (N, T, D_in), where N is the batch size,\n",
    "        T is the length of the sequence(s). src_len, size of (N,), is the valid length for each sequence.\n",
    "        \n",
    "      state: tuple of tensors (h, c). h, size of (N, hidden_size) is the hidden state of LSTM. c, size of \n",
    "            (N, hidden_size), is the memory cell of the LSTM.\n",
    "      \n",
    "    Outputs:\n",
    "      o: tensor of size (N, T, hidden_size).\n",
    "      state: the same as input state.\n",
    "    \"\"\"\n",
    "    \n",
    "    src, src_len = X\n",
    "    h, c = state\n",
    "\n",
    "    # make sure always has a T dim\n",
    "    if len(src.shape) == 2:\n",
    "      src = src.unsqueeze(1)\n",
    "\n",
    "    N, T, D_in = src.shape\n",
    "    W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o = self.params\n",
    "    \n",
    "    o = []\n",
    "    ##############################################################################\n",
    "    # TODO: Implement the forward pass of the LSTM.\n",
    "    ##############################################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "    for t in range(T):\n",
    "      x = src[:,t,:]\n",
    "      forget_gate = torch.sigmoid(x @ W_xf + h @ W_hf + b_f)\n",
    "      input_gate = torch.sigmoid(x @ W_xi + h @ W_hf + b_i)\n",
    "      output_gate = torch.sigmoid(x @ W_xo + h @ W_ho + b_o)\n",
    "      C_tilda = torch.tanh(x @ W_xc + h @ W_hc + b_c)\n",
    "      c = forget_gate * c + input_gate * C_tilda\n",
    "      h = output_gate * torch.tanh(c)\n",
    "      o.append(h.unsqueeze(0))\n",
    "    hidden_seq = torch.cat(o, dim=0)\n",
    "    hidden_seq = hidden_seq.permute(1, 0, 2)\n",
    "    o = o[np.arange(N), src_len]\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "    state = (h, c)\n",
    "    return o, state\n",
    "  \n",
    "  def forward(self, inputs, state):\n",
    "    return self.lstm(inputs, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
